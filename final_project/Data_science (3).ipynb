{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveFeatureEngineering:\n",
    "    def __init__(self, model, num_of_features=10, importance_threshold=0):\n",
    "        \"\"\"\n",
    "        Initializes the RecursiveFeatureEngineering class.\n",
    "        \n",
    "        Parameters:\n",
    "        - model: The machine learning model to be used for evaluating feature importance.\n",
    "        - num_of_features: The number of features to generate during the recursive process.\n",
    "        - importance_threshold: The threshold for feature importance; features with lower importance will be pruned.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.num_of_features = num_of_features\n",
    "        self.importance_threshold = importance_threshold\n",
    "        \n",
    "        # transformations that apply to a single feature\n",
    "        self.single_transformations = [\n",
    "            self.create_log_feature,           \n",
    "            self.create_polynomial_2_feature,    \n",
    "            self.create_polynomial_3_feature,    \n",
    "            self.create_polynomial_4_feature,  \n",
    "            self.create_exponential_feature,    \n",
    "            self.create_square_root_feature,     \n",
    "            self.create_inverse_feature,        \n",
    "            self.create_absolute_feature,            \n",
    "            self.create_root_log_feature         \n",
    "        ]\n",
    "        \n",
    "        # transformations that apply to pairs of features\n",
    "        self.double_transformations = [\n",
    "            self.create_multiplication_feature, \n",
    "            self.create_division_feature, \n",
    "            self.create_addition_feature, \n",
    "            self.create_subtraction_feature, \n",
    "            self.create_polynomial_feature\n",
    "        ]\n",
    "\n",
    "\n",
    "    def create_polynomial_2_feature(self, X, feature):\n",
    "        \"\"\" Creates a degree 2 polynomial feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        clipped_feature = np.clip(X[feature], -1e6, 1e6)  # Prevent overflow\n",
    "        poly_features = poly.fit_transform(clipped_feature.values.reshape(-1, 1))\n",
    "        new_X[f'{feature}^2'] = np.clip(poly_features[:, 1], -1e10, 1e10)  \n",
    "        return new_X\n",
    "\n",
    "    def create_polynomial_3_feature(self, X, feature):\n",
    "        \"\"\" Creates a degree 3 polynomial feature  \"\"\"\n",
    "        new_X = X.copy()\n",
    "        poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "        clipped_feature = np.clip(X[feature], -1e6, 1e6)  # Prevent overflow\n",
    "        poly_features = poly.fit_transform(clipped_feature.values.reshape(-1, 1))\n",
    "        new_X[f'{feature}^3'] = np.clip(poly_features[:, 2], -1e10, 1e10) \n",
    "        return new_X\n",
    "\n",
    "    def create_polynomial_4_feature(self, X, feature):\n",
    "        \"\"\" Creates a degree 4 polynomial feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        poly = PolynomialFeatures(degree=4, include_bias=False)\n",
    "        clipped_feature = np.clip(X[feature], -1e6, 1e6)  # Prevent overflow\n",
    "        poly_features = poly.fit_transform(clipped_feature.values.reshape(-1, 1))\n",
    "        new_X[f'{feature}^4'] = np.clip(poly_features[:, 3], -1e10, 1e10) \n",
    "        return new_X\n",
    "\n",
    "\n",
    "    def create_log_feature(self, X, feature):\n",
    "        \"\"\" Creates a logarithmic feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        valid_mask = (X[feature] > 0) & X[feature].notna() # ensuring only positive values are transformed.\n",
    "        new_X.loc[valid_mask, f'log_{feature}'] = np.log1p(X.loc[valid_mask, feature]) \n",
    "        new_X.loc[~valid_mask, f'log_{feature}'] = 0  \n",
    "        return new_X\n",
    "\n",
    "    def create_exponential_feature(self, X, feature):\n",
    "        \"\"\" Creates an exponential feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        max_val = np.log(1e10)  # Prevents overflow\n",
    "        clipped_feature = np.clip(X[feature], -max_val, max_val)\n",
    "        new_X[f'exp_{feature}'] = np.exp(clipped_feature)\n",
    "        return new_X\n",
    "\n",
    "    def create_inverse_feature(self, X, feature):\n",
    "        \"\"\" Creates an inverse feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        safe_values = np.where(np.abs(X[feature]) < 1e-5, 1e-5, X[feature])  # Replace near zero values\n",
    "        new_X[f'inverse_{feature}'] = 1 / safe_values\n",
    "        return new_X\n",
    "\n",
    "    def create_absolute_feature(self, X, feature):\n",
    "        \"\"\" Creates an absolute value feature. \"\"\"\n",
    "        new_X = X.copy()\n",
    "        new_X[f'abs_{feature}'] = np.abs(X[feature])\n",
    "        return new_X\n",
    "\n",
    "    def create_square_root_feature(self, X, feature):\n",
    "        \"\"\" Creates a square root feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        new_X[f'sqrt_{feature}'] = np.sqrt(np.maximum(X[feature], 0))\n",
    "        return new_X\n",
    "\n",
    "    def create_root_log_feature(self, X, feature):\n",
    "        \"\"\" Creates a root log feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        valid_mask = (X[feature] > 0) & X[feature].notna()\n",
    "        new_X.loc[valid_mask, f'root_log_{feature}'] = np.sqrt(np.log1p(X.loc[valid_mask, feature]))\n",
    "        new_X.loc[~valid_mask, f'root_log_{feature}'] = 0  # Assign 0 to invalid values\n",
    "        return new_X\n",
    "\n",
    "    def create_multiplication_feature(self, X, feature1, feature2):\n",
    "        \"\"\" Creates a multiplication feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        new_X[f\"{feature1}_times_{feature2}\"] = np.clip(X[feature1] * X[feature2], -1e10, 1e10)\n",
    "        return new_X\n",
    "\n",
    "    def create_division_feature(self, X, feature1, feature2):\n",
    "        \"\"\" Creates a division feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        safe_values = np.where(np.abs(X[feature2]) < 1e-5, 1e-5, X[feature2])  # Prevent division by zero\n",
    "        new_X[f\"{feature1}_divided_by_{feature2}\"] = X[feature1] / safe_values\n",
    "        return new_X\n",
    "\n",
    "    def create_addition_feature(self, X, feature1, feature2):\n",
    "        \"\"\" Creates an addition feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        new_X[f\"{feature1}_plus_{feature2}\"] = np.clip(X[feature1] + X[feature2], -1e10, 1e10)\n",
    "        return new_X\n",
    "\n",
    "    def create_subtraction_feature(self, X, feature1, feature2):\n",
    "        \"\"\" Creates a subtraction feature \"\"\"\n",
    "        new_X = X.copy()\n",
    "        new_X[f\"{feature1}_minus_{feature2}\"] = np.clip(X[feature1] - X[feature2], -1e10, 1e10)\n",
    "        return new_X\n",
    "\n",
    "    def create_polynomial_feature(self, X, feature1, feature2):\n",
    "        \"\"\" Creates a polynomial feature of two variables \"\"\"\n",
    "        new_X = X.copy()\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        clipped_features = np.clip(X[[feature1, feature2]], -1e6, 1e6)\n",
    "        poly_features = poly.fit_transform(clipped_features)\n",
    "        new_X[f\"poly_{feature1}_and_{feature2}\"] = np.clip(poly_features.sum(axis=1), -1e10, 1e10)\n",
    "        return new_X\n",
    "\n",
    "    def add_single_feature(self, X, y, feature_importance, score):\n",
    "        \"\"\"\n",
    "        Recursively adds a single feature transformation\n",
    "        \n",
    "        Parameters:\n",
    "        - X: The feature matrix.\n",
    "        - y: The target variable.\n",
    "        - feature_importance: The importance of each feature\n",
    "        - score: The current model score\n",
    "        \n",
    "        Returns:\n",
    "        - X (modified feature matrix), new score (model evaluation score).\n",
    "        \"\"\"\n",
    "        if not self.single_transformations: # Stopping Criterion for the recursion\n",
    "            return X, score\n",
    "\n",
    "        random_transform = random.choice(self.single_transformations)  # Randomly select a transformation\n",
    "\n",
    "        feature_importance = feature_importance.sort_values(ascending=False)\n",
    "        for feature, _ in feature_importance.items():\n",
    "            new_X = random_transform(X, feature)  # Apply transformation\n",
    "            if new_X.columns.equals(X.columns):  # Skip if no new feature is created\n",
    "                continue\n",
    "            _, new_score = self.evaluate_features(new_X, y) # evalute the performance of the model on the new set\n",
    "            if new_score < score:\n",
    "                return new_X, new_score  # Keep new feature if it improves performance\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        self.single_transformations.remove(random_transform)  # Remove ineffective transformation\n",
    "        return self.add_single_feature(X, y, feature_importance, score) # continue to the next transformation \n",
    "\n",
    "    def add_double_feature(self, X, y, score):\n",
    "        \"\"\"\n",
    "        Recursively adds a double feature transformation\n",
    "        \n",
    "        Parameters:\n",
    "        - X: The feature matrix.\n",
    "        - y: The target variable.\n",
    "        - score: The current model score.\n",
    "        \n",
    "        Returns:\n",
    "        - X (modified feature matrix), new score (model evaluation score).\n",
    "        \"\"\"\n",
    "        if not self.double_transformations: # Stopping Criterion for the recursion\n",
    "            return X, score\n",
    "        \n",
    "        random_transform = random.choice(self.double_transformations) # Randomly select a transformation\n",
    "\n",
    "        # compute a Corralation matrix to order the feature cuples\n",
    "        Corralation_matrix = X.corr().abs()\n",
    "        Corralation_matrix = Corralation_matrix.unstack().sort_values(ascending=False)\n",
    "        Corralation_matrix = Corralation_matrix[Corralation_matrix.index.get_level_values(0) != Corralation_matrix.index.get_level_values(1)] \n",
    "\n",
    "        for feature1, feature2 in Corralation_matrix.index:\n",
    "            if feature1 in X.columns and feature2 in X.columns:\n",
    "                new_X = random_transform(X, feature1, feature2) # Apply transformation \n",
    "                if new_X.columns.equals(X.columns): # Skip if no new feature is created\n",
    "                    continue\n",
    "                _, new_score = self.evaluate_features(new_X, y) # evalute the performance of the model on the new set\n",
    "                if new_score < score:\n",
    "                    return new_X, new_score # Keep new feature if it improves performance\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        self.double_transformations.remove(random_transform) # Remove ineffective transformation\n",
    "        return self.add_double_feature(X, y,score) # continue to the next transformation \n",
    "\n",
    "    def generate_feature_combinations(self, X, y):\n",
    "        \"\"\"\n",
    "        Generates combinations of features for recursive feature engineering.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: The feature matrix.\n",
    "        - y: The target variable.\n",
    "        \n",
    "        Returns:\n",
    "        - X (modified feature matrix), score (model evaluation score).\n",
    "        \"\"\"\n",
    "        feature_importance, score = self.evaluate_features(X, y) # evalute the model for a base score\n",
    "        num_features = random.randint(1, 2)  # Decide whether to do a one or two feature transformation\n",
    "\n",
    "        if num_features == 1:\n",
    "            return self.add_single_feature(X, y, feature_importance, score)\n",
    "        else:\n",
    "            return self.add_double_feature(X, y, score)\n",
    "\n",
    "    def prune_features(self, X, y):\n",
    "        \"\"\"\n",
    "        Prunes features that are below the importance threshold.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: The feature matrix.\n",
    "        - y: The target variable.\n",
    "        \n",
    "        Returns:\n",
    "        - X (modified feature matrix), score (model evaluation score).\n",
    "        \"\"\"\n",
    "        feature_importance, score = self.evaluate_features(X, y) # evalute the model for a base score and feature importance table\n",
    "        feature_importance = feature_importance[feature_importance > self.importance_threshold] # remove the feature with low importamce \n",
    "        new_X = X[feature_importance.index]\n",
    "        _, new_score = self.evaluate_features(new_X, y)\n",
    "        if(new_score < score):\n",
    "            return new_X, new_score # if the model performed better, keep the change\n",
    "        else:\n",
    "            return X, score\n",
    "\n",
    "    def evaluate_features(self, X, y):\n",
    "        \"\"\"\n",
    "        Evaluates the model performance based on the feature matrix and target variable.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: The feature matrix.\n",
    "        - y: The target variable.\n",
    "        \n",
    "        Returns:\n",
    "        - feature_importances: The importance of each feature.\n",
    "        - score: The model evaluation score (RMSE).\n",
    "        \"\"\"\n",
    "        X = X.select_dtypes(include=['number']).copy()\n",
    "        X.replace([np.inf, -np.inf], np.nan, inplace=True)  # Handle infinite values\n",
    "        X.fillna(X.mean(numeric_only=True), inplace=True)  # Fill missing values with column mean\n",
    "        X = X.astype(np.float32)  # Ensure numerical consistency\n",
    "        y = y.fillna(y.mean()).astype(np.float32)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split the data\n",
    "        self.model.fit(X_train, y_train)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        score = np.sqrt(mean_squared_error(y_test, y_pred))  # Compute RMSE\n",
    "\n",
    "        feature_importances = pd.Series(self.model.feature_importances_, index=X.columns)\n",
    "        return feature_importances, score\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        Performs recursive feature engineering and returns the best feature set.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: The feature matrix.\n",
    "        - y: The target variable.\n",
    "        \n",
    "        Returns:\n",
    "        - X (final feature matrix), best_score (model evaluation score).\n",
    "        \"\"\"\n",
    "        print(\"Starting recursive feature engineering...\")\n",
    "        X = X.select_dtypes(include=['number']).copy()\n",
    "        X.replace([np.inf, -np.inf], np.nan, inplace=True)  # Handle infinite values\n",
    "        X.fillna(X.mean(numeric_only=True), inplace=True)  # Fill missing values with column mean\n",
    "        X = X.astype(np.float32)  # Ensure numerical consistency\n",
    "        y = y.fillna(y.mean()).astype(np.float32)\n",
    "\n",
    "        iteration = 0\n",
    "        best_score = float('inf')\n",
    "\n",
    "        while iteration < self.num_of_features: # until the right amount of features generated or no improvment is found\n",
    "            print(f'Iteration {iteration + 1}')\n",
    "            X_expanded, score_expanded = self.generate_feature_combinations(X, y) # generate a new feature transformation\n",
    "            X_expanded, score_expanded = self.prune_features(X_expanded, y) # remove features with low importance\n",
    "\n",
    "            if score_expanded < best_score: # if we found a cafiguration that improves the performance, keep it\n",
    "                print(f\"New best score found: {score_expanded}\")\n",
    "                best_score = score_expanded\n",
    "                X = X_expanded.copy()\n",
    "            else: # if we didnt found a transformation that will improve the perforamne\n",
    "                print(\"No improvement detected. Stopping feature engineering.\")\n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        print(\"Feature engineering completed.\")\n",
    "        return X, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(random_state=42)\n",
    "rfe = RecursiveFeatureEngineering(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base score: 4002.8788631292314\n",
      "Starting recursive feature engineering...\n",
      "Iteration 1\n",
      "New best score found: 4000.6772297669586\n",
      "Iteration 2\n",
      "New best score found: 3990.7347547289887\n",
      "Iteration 3\n",
      "New best score found: 3987.5941654799026\n",
      "Iteration 4\n",
      "New best score found: 3957.129540605996\n",
      "Iteration 5\n",
      "New best score found: 3953.8311149969277\n",
      "Iteration 6\n",
      "New best score found: 3944.9534953092807\n",
      "Iteration 7\n",
      "New best score found: 3941.478939378808\n",
      "Iteration 8\n",
      "New best score found: 3931.8319554487784\n",
      "Iteration 9\n",
      "New best score found: 3920.8278337529205\n",
      "Iteration 10\n",
      "New best score found: 3917.2264996448544\n",
      "Feature engineering completed.\n",
      "2.139769061545337%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/ParisHousing.csv')\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "_, base_score = rfe.evaluate_features(X, y)\n",
    "print(f'base score: {base_score}')\n",
    "X_transformed, score = rfe.fit_transform(X, y)\n",
    "print(f'{((base_score - score)/base_score)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base score: 0.025892922451380078\n",
      "Starting recursive feature engineering...\n",
      "Iteration 1\n",
      "New best score found: 0.02587251599830059\n",
      "Iteration 2\n",
      "New best score found: 0.02561289463888072\n",
      "Iteration 3\n",
      "New best score found: 0.025131045871464204\n",
      "Iteration 4\n",
      "New best score found: 0.025114505317206343\n",
      "Iteration 5\n",
      "New best score found: 0.025107274993190876\n",
      "Iteration 6\n",
      "New best score found: 0.025098672269700036\n",
      "Iteration 7\n",
      "New best score found: 0.02509601367360587\n",
      "Iteration 8\n",
      "New best score found: 0.025050208779318257\n",
      "Iteration 9\n",
      "New best score found: 0.025047243235558105\n",
      "Iteration 10\n",
      "New best score found: 0.025039544469216076\n",
      "Feature engineering completed.\n",
      "3.295796307915475%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/flood.csv')\n",
    "X = df.drop(columns=['FloodProbability'])\n",
    "y = df['FloodProbability']\n",
    "_, base_score = rfe.evaluate_features(X, y)\n",
    "print(f'base score: {base_score}')\n",
    "X_transformed, score = rfe.fit_transform(X, y)\n",
    "print(f'{((base_score - score)/base_score)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base score: 0.05492278234859448\n",
      "Starting recursive feature engineering...\n",
      "Iteration 1\n",
      "New best score found: 0.05476747409223273\n",
      "Iteration 2\n",
      "New best score found: 0.05447228539661956\n",
      "Iteration 3\n",
      "New best score found: 0.05443666743477979\n",
      "Iteration 4\n",
      "New best score found: 0.05429413713100989\n",
      "Iteration 5\n",
      "New best score found: 0.05427390411843533\n",
      "Iteration 6\n",
      "New best score found: 0.05423340327022739\n",
      "Iteration 7\n",
      "New best score found: 0.05415875100248315\n",
      "Iteration 8\n",
      "New best score found: 0.05393790168982996\n",
      "Iteration 9\n",
      "New best score found: 0.053822974722991995\n",
      "Iteration 10\n",
      "New best score found: 0.05376990244621762\n",
      "Feature engineering completed.\n",
      "2.099092313021477%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/weatherHistory.csv')\n",
    "X = df.drop(columns=['Apparent Temperature (C)'])\n",
    "y = df['Apparent Temperature (C)']\n",
    "_, base_score = rfe.evaluate_features(X, y)\n",
    "print(f'base score: {base_score}')\n",
    "X_transformed, score = rfe.fit_transform(X, y)\n",
    "print(f'{((base_score - score)/base_score)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base score: 19827.827277120257\n",
      "Starting recursive feature engineering...\n",
      "Iteration 1\n",
      "New best score found: 19790.713653412746\n",
      "Iteration 2\n",
      "New best score found: 19774.610265146643\n",
      "Iteration 3\n",
      "New best score found: 19751.151642948502\n",
      "Iteration 4\n",
      "New best score found: 19730.430981122565\n",
      "Iteration 5\n",
      "New best score found: 19648.214240693567\n",
      "Iteration 6\n",
      "New best score found: 19549.95451033402\n",
      "Iteration 7\n",
      "New best score found: 18596.268127014726\n",
      "Iteration 8\n",
      "New best score found: 18545.033981619705\n",
      "Iteration 9\n",
      "New best score found: 18538.952691351114\n",
      "Iteration 10\n",
      "New best score found: 18510.230689902644\n",
      "Feature engineering completed.\n",
      "6.645188949865501%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/house_price_regression_dataset.csv')\n",
    "X = df.drop(columns=['House_Price'])\n",
    "y = df['House_Price']\n",
    "_, base_score = rfe.evaluate_features(X, y)\n",
    "print(f'base score: {base_score}')\n",
    "X_transformed, score = rfe.fit_transform(X, y)\n",
    "print(f'{((base_score - score)/base_score)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base score: 514.0469090426051\n",
      "Starting recursive feature engineering...\n",
      "Iteration 1\n",
      "New best score found: 512.2237188407865\n",
      "Iteration 2\n",
      "New best score found: 510.4883649130336\n",
      "Iteration 3\n",
      "New best score found: 507.49239968102927\n",
      "Iteration 4\n",
      "New best score found: 496.8286689097764\n",
      "Iteration 5\n",
      "New best score found: 496.0888666736249\n",
      "Iteration 6\n",
      "New best score found: 495.3012683073736\n",
      "Iteration 7\n",
      "New best score found: 491.19909673059993\n",
      "Iteration 8\n",
      "New best score found: 487.2643523754238\n",
      "Iteration 9\n",
      "New best score found: 481.542050062204\n",
      "Iteration 10\n",
      "New best score found: 481.23128263062296\n",
      "Feature engineering completed.\n",
      "6.383780513941834%\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/test_energy_data.csv')\n",
    "X = df.drop(columns=['Energy Consumption'])\n",
    "y = df['Energy Consumption']\n",
    "_, base_score = rfe.evaluate_features(X, y)\n",
    "print(f'base score: {base_score}')\n",
    "X_transformed, score = rfe.fit_transform(X, y)\n",
    "print(f'{((base_score - score)/base_score)*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
